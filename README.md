# Language Processing Analyzer

This repository contains the development of a **Language Processing Analyzer**, structured into three phases. This document currently focuses on **Phase 1: Lexical Analysis**, with placeholders for the upcoming phases.

## Phase 1: Lexical Analysis

### What is a Lexical Analyzer?
A **Lexical Analyzer (Lexer)** is the first stage of a compiler or interpreter. Its function is to **take the source code as input and break it down into the smallest meaningful units, called tokens**.

**Example of source code:**
```c
int x = 10;
```
**Lexical Analyzer Output:**
```
TOKEN_KEYWORD(int)
TOKEN_IDENTIFIER(x)
TOKEN_OPERATOR(=)
TOKEN_NUMBER(10)
TOKEN_SYMBOL(;)
```
These tokens will be used in the next phase (**syntax analysis**) to verify the structure of the code.

### How does it work?
1. **Reads the source code.**
2. **Ignores whitespace and comments.**
3. **Identifies tokens according to lexical rules (regular expressions).**
4. **Generates a list of tokens.**
5. **Passes the tokens to the parser.**

### Understanding Greedy Regular Expressions
In lexical analysis, regular expressions are **greedy** by default, meaning they try to match the longest possible string that fits their pattern. Here's how it works:

1. The lexer starts at a position in the input.
2. It looks ahead character by character, trying to match the longest possible sequence.
3. When it can't match any more characters, it creates a token with the matched sequence.

**Example with numbers:**
```python
# For the regular expression '\d+' (one or more digits)
Input: "123abc"

Lexer process:
1. Starts at '1' â†’ matches
2. Looks ahead to '2' â†’ still matches
3. Looks ahead to '3' â†’ still matches
4. Looks ahead to 'a' â†’ doesn't match
5. Creates NUMBER token with "123"
```

This greedy behavior ensures that numbers like "123" are tokenized as a single NUMBER token (123) rather than three separate tokens (1, 2, 3).

## Automata and Regular Expressions
The **lexer** can be implemented using **Deterministic Finite Automata (DFA)**, generated from **Regular Expressions**.

**Example of a regular expression for identifiers:**
```
[a-zA-Z][a-zA-Z0-9_]*
```
This represents a **word that starts with a letter and can contain numbers and underscores**.

**Example of a DFA to recognize "if", "int", and "else":**
```
  (q0) -- 'i' --> (q1) -- 'f' --> (q2) [IF]
    |                        
    |-- 'n' --> (q3) -- 't' --> (q4) [INT]
    |-- 'e' --> (q5) -- 'l' --> (q6) -- 's' --> (q7) -- 'e' --> (q8) [ELSE]
```
This diagram shows how a DFA recognizes keywords by following transitions between states.

## Implementation with PLY
In this repository, a lexer is implemented using PLY, utilizing **regular expressions and functions in Python** to define tokens.

**Example of lexer code:**
```python
import ply.lex as lex

# Token list
tokens = ['IDENTIFIER', 'NUMBER', 'IF', 'ELSE', 'PLUS', 'EQUAL']

# Lexical rules
t_IDENTIFIER = r'[a-zA-Z_][a-zA-Z0-9_]*'
t_NUMBER = r'\d+'
t_IF = r'if'
t_ELSE = r'else'
t_PLUS = r'\+'
t_EQUAL = r'='

def t_error(t):
    print(f"Illegal character: {t.value[0]}")
    t.lexer.skip(1)

lexer = lex.lex()
```

## Phase 2: Syntax Analysis (Parser)

### What is a Syntax Analyzer (Parser)?
A **Syntax Analyzer** (also called a **Parser**) is the second stage of a compiler or interpreter. It takes the tokens generated by the lexical analyzer (lexer) and checks if they conform to the grammatical structure of the language. The output of the parser is typically an **Abstract Syntax Tree (AST)**, which represents the hierarchical structure of the program according to its syntax rules.

**Example of source code:**
```plaintext
x := 10 + 2
```

**Parser Output (Abstract Syntax Tree):**
```
ASSIGN
â”œâ”€â”€ ID (x)
â””â”€â”€ ADDITION
    â”œâ”€â”€ NUMBER (10)
    â””â”€â”€ NUMBER (2)
```

This output structure shows that the statement is an assignment (`ASSIGN`), where `x` is assigned the result of the addition of `10` and `2`.

### How does it work?
1. **Receives tokens** generated by the lexer.
2. **Verifies if the tokens match the grammar rules**.
3. **Builds an Abstract Syntax Tree (AST)** to represent the program's structure.
4. **Checks for syntax errors** and reports them if any are found.

### Grammar Rules
Grammar rules define how the language constructs should be structured. Each rule is associated with a function that processes the matched tokens.

**Example of a grammar rule for an assignment statement:**
```
stmt : ID ASSIGN expr
```
This rule says that a `stmt` (statement) consists of an `ID` (identifier), followed by an `ASSIGN` token, followed by an `expr` (expression).

### Implementation with PLY
The parser is implemented using **PLY** (Python Lex-Yacc), a library that provides tools for lexical analysis and parsing. In PLY, you define grammar rules and associate them with functions that process matching tokens.

**Example of Parser Code:**

```python
import ply.yacc as yacc

# Define precedence for operators
precedence = (
    ('left', 'PLUS'),
)

# Grammar rules and corresponding functions
def p_stmt_assign(p):
    '''stmt : ID ASSIGN expr'''
    print(f"Assignment: {p[1]} = {p[3]}")

def p_expr_number(p):
    '''expr : NUMBER'''
    p[0] = int(p[1])

def p_expr_add(p):
    '''expr : expr PLUS expr'''
    p[0] = p[1] + p[3]

def p_error(p):
    if p:
        print(f"Syntax error in input: {p.value} at line {p.lineno}")
    else:
        print("Syntax error in input: none.")

# Build the parser
parser = yacc.yacc()
```

### How the Parser Works
1. The parser starts reading the tokens generated by the lexer.
2. It checks if the tokens match any of the grammar rules.
3. When a match is found, the associated function is executed, and the result is stored or processed.
4. In the example above, for an input like `x := 10 + 2`, the parser will match the rule `stmt : ID ASSIGN expr` and process it.
5. The parser builds an **Abstract Syntax Tree (AST)** to represent the structure of the expression.

### Operator Precedence
In the parser, we also define the **precedence** of operators to resolve ambiguities in expressions. For example, we define that addition (`PLUS`) has lower precedence than multiplication (`TIMES`):

```python
precedence = (
    ('left', 'PLUS'),
    ('left', 'TIMES'),
)
```

This ensures that multiplication is performed before addition in expressions like `x + y * z`.

---

## Phase 3: Intermediate Code Generation
ðŸ”œ **To be implemented in the final assignment.**

## Phase 3: Intermediate Code Generation
ðŸ”œ **To be implemented in the final assignment.**


